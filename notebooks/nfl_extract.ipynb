{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "526dd1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e9aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('d:/myprojects/project_api_data_orchestrator/src')\n",
    "\n",
    "%run ../src/project_api_data_orchestrator/core/config.py\n",
    "%run ../src/project_api_data_orchestrator/db/connection.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Scrape Depth Chart Functions\n",
    "BASE_URL = \"https://www.ourlads.com\"\n",
    "DEPTHCHARTS_INDEX = BASE_URL + \"/nfldepthcharts/depthcharts.aspx\"\n",
    "\n",
    "# Target positions (including variants for WR)\n",
    "TARGET_POS = {\"QB\", \"WR\", \"RB\", \"TE\", \"LWR\", \"RWR\", \"SWR\"}\n",
    "\n",
    "def get_team_links():\n",
    "    resp = requests.get(DEPTHCHARTS_INDEX, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if href.startswith(\"/nfldepthcharts/depthchart/\"):\n",
    "            full = BASE_URL + href\n",
    "            if full not in links:\n",
    "                links.append(full)\n",
    "    return links\n",
    "\n",
    "def parse_team_depthchart(team_url):\n",
    "    resp = requests.get(team_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # Derive team abbreviation\n",
    "    team_abbrev = None\n",
    "    depwrapper = soup.find(\"div\", id=\"ctl00_phContent_DepWrapper\")\n",
    "    if depwrapper:\n",
    "        for c in depwrapper.get(\"class\", []):\n",
    "            if c.startswith(\"dt-\"):\n",
    "                team_abbrev = c[3:]\n",
    "                break\n",
    "    if not team_abbrev:\n",
    "        team_abbrev = team_url.rstrip(\"/\").split(\"/\")[-1].upper()\n",
    "\n",
    "    records = []\n",
    "    # Locate the table\n",
    "    table = depwrapper.find(\"table\", class_=\"table-bordered\") if depwrapper else None\n",
    "    if table is None:\n",
    "        table = soup.find(\"table\", class_=\"table-bordered\")\n",
    "    if table is None:\n",
    "        return records\n",
    "\n",
    "    tbody = table.find(\"tbody\") or table.find(\"tbody\", id=\"ctl00_phContent_dcTBody\")\n",
    "    if tbody is None:\n",
    "        return records\n",
    "\n",
    "    for tr in tbody.find_all(\"tr\"):\n",
    "        tds = tr.find_all(\"td\")\n",
    "        if len(tds) < 2:\n",
    "            continue\n",
    "\n",
    "        pos_raw = tds[0].get_text(strip=True)\n",
    "        pos = pos_raw.strip()\n",
    "        pos_norm = \"WR\" if pos in (\"LWR\", \"RWR\", \"SWR\") else pos\n",
    "\n",
    "        if pos_norm not in TARGET_POS:\n",
    "            continue\n",
    "\n",
    "        tier = 1\n",
    "        for num_idx in range(1, len(tds), 2):\n",
    "            player_idx = num_idx + 1\n",
    "            if player_idx >= len(tds):\n",
    "                break\n",
    "            a = tds[player_idx].find(\"a\")\n",
    "            if a and a.get_text(strip=True):\n",
    "                player_text = a.get_text(strip=True)\n",
    "                player_text = re.sub(\n",
    "                    r\"\\s+(?:[A-Z]{2}\\d{2}|\\d{2}/\\d|[A-Z]{1,2}/[A-Za-z]{2,3})$\",\n",
    "                    \"\",\n",
    "                    player_text,\n",
    "                    flags=re.IGNORECASE,\n",
    "                )\n",
    "                player_clean = player_text.title().strip()\n",
    "                records.append((player_clean, team_abbrev, pos_norm, tier))\n",
    "            tier += 1\n",
    "\n",
    "    return records\n",
    "\n",
    "def scrape_all_to_dataframe():\n",
    "    team_links = get_team_links()\n",
    "    print(f\"Found {len(team_links)} team pages.\")\n",
    "    all_records = []\n",
    "    for link in team_links:\n",
    "        try:\n",
    "            recs = parse_team_depthchart(link)\n",
    "            # print(f\"  {link} â†’ {len(recs)} records\")\n",
    "            all_records.extend(recs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {link}: {e}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Deduplicate\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for rec in all_records:\n",
    "        if rec not in seen:\n",
    "            seen.add(rec)\n",
    "            deduped.append(rec)\n",
    "\n",
    "    df = pd.DataFrame(deduped, columns=[\"Player\", \"Team\", \"Position\", \"Tier\"])\n",
    "    print(f\"Scraped {len(df)} unique player-position records.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d77da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 team pages.\n",
      "Scraped 469 unique player-position records.\n",
      "            Player Team Position  Tier\n",
      "0   Palmer, Joshua  BUF       WR     1\n",
      "1  Shavers, Tyrell  BUF       WR     2\n",
      "2    Coleman, Keon  BUF       WR     1\n",
      "3   Samuel, Curtis  BUF       WR     2\n",
      "4   Shakir, Khalil  BUF       WR     1\n"
     ]
    }
   ],
   "source": [
    "df = scrape_all_to_dataframe()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40fe0ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARZ',\n",
       " 'ATL',\n",
       " 'BAL',\n",
       " 'BUF',\n",
       " 'CAR',\n",
       " 'CHI',\n",
       " 'CIN',\n",
       " 'CLE',\n",
       " 'DAL',\n",
       " 'DEN',\n",
       " 'DET',\n",
       " 'GB',\n",
       " 'HOU',\n",
       " 'IND',\n",
       " 'JAX',\n",
       " 'KC',\n",
       " 'LAC',\n",
       " 'LAR',\n",
       " 'LV',\n",
       " 'MIA',\n",
       " 'MIN',\n",
       " 'NE',\n",
       " 'NO',\n",
       " 'NYG',\n",
       " 'NYJ',\n",
       " 'PHI',\n",
       " 'PIT',\n",
       " 'SEA',\n",
       " 'SF',\n",
       " 'TB',\n",
       " 'TEN',\n",
       " 'WAS']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sorted(df['Team'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c364e799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23388\\1316146615.py:4: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_pg = pd.read_sql_query(query, conn)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conn = get_connection('nfl_data')\n",
    "\n",
    "query = \"SELECT * FROM public.teams ORDER BY team_id ASC\"\n",
    "df_pg = pd.read_sql_query(query, conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f9067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Check if Teams match\n",
    "sorted(df['Team'].unique()) == sorted(df_pg['abbreviation'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff42afa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-api-data-orchestrator-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
